<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xiaoyu Liu</title>

    <meta name="author" content="Xiaoyu Liu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody><tr style="padding:0px"><td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Xiaoyu Liu
                </p>
                <p>
                I am an Assistant Professor in the <a href="https://www.usf.edu/engineering/me/index.aspx" style="color:#2984D1;">Department of Mechanical and Aerospace Engineering</a> at the University of South Florida. Prior to this, I was a postdoctoral researcher at the NASA-funded <a href="https://www.purdue.edu/rethi/" style="color:#2984D1;">RETH Institute</a>, where I focused on human and robotic agent modeling and analysis in space missions. I hold a Ph.D. and M.S. in Mechanical Engineering from Purdue University, where I had the privilege of being advised by <a href="https://engineering.purdue.edu/ME/People/ptProfile?id=57291" style="color:#2984D1;">Prof. Shirley J. Dyke</a>, and a B.S. in Mechanical Engineering from Xi’an Jiaotong University.
                </p>
                <p style="text-align:center">
                  <a href="https://scholar.google.com/citations?hl=en&user=Uo2HX8oAAAAJ&view_op=list_works&sortby=pubdate" style="color:#2984D1;">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.usf.edu/engineering/me/people/index.aspx#:~:text=Thin%20Film%20Technology-,Xiaoyu%20Liu,-Assistant%20Professor" style="color:#2984D1;">ME@USF</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <img style="width:100%;max-width:100%;object-fit:
                 cover; border-radius: 2%;" alt="profile photo" src="images/Photo.png">
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>About Research</h2>
                <p>
                My main research interests focus on applying machine learning and mathematical methods to address challenges in human–robot teaming for space missions.
                </p>
                <p>
                I am currently recruiting Ph.D. and M.S. students starting in Spring/Fall 2026. If you are interested in machine learning/multi-agent teaming/space mission planning, send me an email at xiaoyuliu@usf.edu!
                </p>
              </td>
            </tr>
          </tbody></table>

          <!--research projects-->          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/Project_agent.png' width=100%>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Computational framework for assessing space mission outcomes with humans and robots</span>
                <br>
                This project presents a fast, customizable computational framework to compare human crews and robotic agents for space missions. By modeling agents as system-of-systems and applying stochstic Markovian process modeling with Monte Carlo simulation, the framework captures agents' work schedules, resource consumption, and task-performance capability, then maps a benefit–cost trade space (Science Research Hours vs. Modified-ESM) to guide mission-ready choices.</br>
                <p>
                <a href="https://arc.aiaa.org/doi/abs/10.2514/1.J063846">paper</a>

                </p>
                <p style="font-size:smaller">Liu, X., Behjat, A., Dyke, S. J., Whitaker, D., Ramirez, J., & Bilionis, I. (2025). Computational Framework for Assessing Mission Outcomes with Humans and Robots. AIAA Journal, 63(4), 1596-1614.</br>
                Liu, X., Behjat, A., Dyke, S., Whitaker, D., Ramirez, J., & Bilionis, I. (2023, July). Roles of human and robotic agents toward operating a smart space habitat. 2023 International Conference on Environmental Systems.
                </p>
              </td>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/Project_simulation.png' width=100%>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Computational framework for making early design decisions in deep space habitats</span>
                <br>
                A modular Control-oriented Dynamic Computational Model is developed for rapid simulation of system-of-systems (e.g., deep-space habitats) with disturbances, damage/degradation, repair/recovery, and a simple health-management decision layer. The Python-based framework outputs decision-ready cost–value trade spaces for early design choices and is demonstrated on structural-maintenance and space-habitat power-configuration case studies. </br>
                <p>
                <a href="https://www.sciencedirect.com/science/article/pii/S0965997824000978">paper</a>

                </p>
                <p style="font-size:smaller">Behjat, A., Liu, X., Forero, O., Ibrahimov, R., Dyke, S., Bilionis, I., ... & Whitaker, D. (2024). A computational framework for making early design decisions in deep space habitats. Advances in Engineering Software, 195, 103690.
                </p>
              </td>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/Project_localization.png' width=100%>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Automated image localization to support rapid building reconnaissance in a large‐scale area</span>
                <br>
                A fully automated indoor image-localization is developed for post-event building reconnaissance. The framework aligns the reconstructed path and point cloud to structural drawings to yield reliable photo locations across varied layouts, validated on multiple real buildings with only inexpensive wearable cameras.</br>
                <p>
                <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/mice.12828">paper</a>

                </p>
                <p style="font-size:smaller">Liu, X., Dyke, S. J., Lenjani, A., Bilionis, I., Zhang, X., & Choi, J. (2023). Automated image localization to support rapid building reconnaissance in a large‐scale area. Computer‐Aided Civil and Infrastructure Engineering, 38(1), 3-25.</br>
                Liu, X., Dyke, S. J., Yeum, C. M., Bilionis, I., Lenjani, A., & Choi, J. (2020). Automated indoor image localization to support a post-event building assessment. Sensors, 20(6), 1610.
                </p>
              </td>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/Project_NBF.png' width=100%>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Accelerated naive Bayesian probability fusion algorithm</span>
                <br>
                This project provides an accelerated naive Bayesian probability fusion algorithm that enables fast computation even when dealing with very large probability lists, where the exact method becomes prohibitively slow. It is part of the work as “Information fusion to automatically classify post-event building damage state”.
                <p>
                <a href="https://github.com/xiaoyuliu-space/Fast_naive_Bayesian_fusion">github</a>
                /
                <a href="https://www.sciencedirect.com/science/article/pii/S0141029621018447?casa_token=7xgRbjXI5hwAAAAA:y4XqbM3h7DXQeROZkjjhHpiBvo9n_ZGnZT3U0cw-fQxwy1uWasG9KZAPVRB7djbLNXXJfPmo">paper</a>

                </p>
                <p style="font-size:smaller">Liu, X., Iturburu, L., Dyke, S. J., Lenjani, A., Ramirez, J., & Zhang, X. (2022). Information fusion to automatically classify post-event building damage state. Engineering Structures, 253, 113765.
                </p>
              </td>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/Project_fusion.png' width=100%>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Information fusion to automatically classify post-event building damage state</span>
                <br>
                This research developed an automated information-fusion framework classifies a building’s overall post-event damage state from sets of field photos by combining image-level classification results. Validated on 29,543 field images from 720 buildings, this framework delivers robust, building-level damage categories to speed up reconnaissance and downstream decision-making.  ￼
                <p>
                <a href="https://www.sciencedirect.com/science/article/pii/S0141029621018447">paper</a>

                </p>
                <p style="font-size:smaller">Liu, X., Iturburu, L., Dyke, S. J., Lenjani, A., Ramirez, J., & Zhang, X. (2022). Information fusion to automatically classify post-event building damage state. Engineering Structures, 253, 113765.
                </p>
              </td>
          </tbody></table>

          <!--bottom annotation-->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Source code from <a href="https://jonbarron.info/">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>

        </td></tr></tbody></table>
  </body>
</html>
